# ğŸ•Šï¸ DOVE-SkyScripts

<div align="center">

**Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-Text Retrieval**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![arXiv](https://img.shields.io/badge/arXiv-2310.08276-b31b1b.svg)](https://arxiv.org/abs/2310.08276)
[![Dataset](https://img.shields.io/badge/Dataset-SkyScript-green.svg)](https://github.com/wangzhecheng/SkyScript)

[English](README.md) | [ä¸­æ–‡](README_zh.md) | [Tiáº¿ng Viá»‡t](README_vi.md)

</div>

---

## ğŸ“– Tá»•ng quan

DOVE (Direction-Oriented Visual-semantic Embedding) lÃ  má»™t framework Ä‘á»™t phÃ¡ cho **remote sensing image-text retrieval**, Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn **SkyScript dataset** - bá»™ dataset vision-language lá»›n nháº¥t vÃ  Ä‘a dáº¡ng nháº¥t cho áº£nh viá»…n thÃ¡m vá»›i **2.6M image-text pairs** vÃ  **29K semantic tags** riÃªng biá»‡t.

### ğŸ¯ Váº¥n Ä‘á» chÃ­nh

Remote sensing image-text retrieval Ä‘á»‘i máº·t vá»›i thá»­ thÃ¡ch **visual-semantic imbalance**:

- **Visual-semantic redundancy**: CÃ¡c váº­t thá»ƒ nhá» dá»… bá»‹ nhiá»…u tá»« background vÃ  irrelevant objects
- **Inter-class similarity**: áº¢nh cá»§a cÃ¡c scene khÃ¡c nhau cÃ³ thá»ƒ ráº¥t giá»‘ng nhau

<div align="center">
<img src="docs/images/visual_semantic_imbalance.png" width="800">
<p><i>HÃ¬nh 1: Visual-semantic imbalance trong remote sensing</i></p>
</div>

### ğŸ’¡ Giáº£i phÃ¡p cá»§a DOVE

DOVE giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch:

1. **Regional-Oriented Attention Module (ROAM)**: Äiá»u chá»‰nh khoáº£ng cÃ¡ch giá»¯a visual vÃ  textual embeddings trong latent space
2. **Digging Text Genome Assistant (DTGA)**: TÄƒng cÆ°á»ng textual representation vá»›i global word-level semantic connections
3. **Global Visual-Semantic Constraint**: Giáº£m single visual dependency vÃ  constraint cho final embeddings

<div align="center">
<img src="docs/images/dove_architecture.png" width="900">
<p><i>HÃ¬nh 2: Kiáº¿n trÃºc tá»•ng thá»ƒ cá»§a DOVE model</i></p>
</div>

---

## âœ¨ TÃ­nh nÄƒng chÃ­nh

### ğŸ”¥ Performance

- âœ… **+6.2% accuracy** so vá»›i baseline CLIP trÃªn zero-shot scene classification
- âœ… **SOTA results** trÃªn RSICD vÃ  RSITMD datasets
- âœ… **Zero-shot transfer** cho fine-grained object attribute classification
- âœ… **Cross-modal retrieval** vá»›i mean recall vÆ°á»£t trá»™i

### ğŸ“Š Dataset: SkyScript

- ğŸ“¸ **2.6M image-text pairs** (5.2M unfiltered)
- ğŸ·ï¸ **29K distinct semantic tags** (44K unfiltered)
- ğŸŒ **Global coverage** tá»« multiple satellite sources
- ğŸ¯ **Multi-resolution**: 0.1m - 30m GSD
- ğŸ”— **Multi-source**: SWISSIMAGE, NAIP, Sentinel-2, Landsat, Planet SkySat

<div align="center">
<img src="docs/images/skyscript_coverage.png" width="800">
<p><i>HÃ¬nh 3: Geographic coverage cá»§a SkyScript dataset</i></p>
</div>

---

## ğŸ—ï¸ Kiáº¿n trÃºc

### DOVE Model Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Input Representation                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ MSV Encoder  â”‚  â”‚  RoI Encoder â”‚  â”‚    DTGA    â”‚ â”‚
â”‚  â”‚  (ResNet-50) â”‚  â”‚  (ResNet-50) â”‚  â”‚ (BiGRU +   â”‚ â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚  Gated SA) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Modality Interaction (ROAM)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Intra-modal Fusion   â”‚  â”‚ Inter-modal Guidanceâ”‚  â”‚
â”‚  â”‚   Attention (IFA)    â”‚  â”‚   Attention (IGA)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Similarity Measurement & Loss             â”‚
â”‚  â€¢ Ranking Loss: L(V_MR, T_RG)                      â”‚
â”‚  â€¢ Global Constraint: Î»_g Ã— L(V_M, T_G)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Modules

#### 1. **DTGA (Digging Text Genome Assistant)**

```python
# Dual-branch structure Ä‘á»ƒ enhance textual features
H_f = GRU_forward(text_embeddings)
H_b = GRU_backward(text_embeddings)

# Gated self-attention
H_f_tilde = GatedSelfAttention(H_f)
H_b_tilde = GatedSelfAttention(H_b)

# Interactive features
T_fâŠ™b = T_f âŠ™ Probability(H_b)
T_bâŠ™f = T_b âŠ™ Probability(H_f)

# Final textual features
F_G = MLP(T_fâŠ™b + T_bâŠ™f)
```

#### 2. **ROAM (Regional-Oriented Attention Module)**

- **IFA (Intra-modal Fusion Attention)**: Fuse multiscale vÃ  regional visual features
- **IGA (Inter-modal Guidance Attention)**: Guide textual features báº±ng regional visual features

---

## ğŸš€ CÃ i Ä‘áº·t

### Requirements

- Python >= 3.8
- CUDA >= 11.8 (recommended)
- 16GB+ RAM
- NVIDIA GPU vá»›i 8GB+ VRAM (16GB+ recommended)

### 1. Clone Repository

```bash
git clone https://github.com/AlanKhan145/Dove-SkyScripts.git
cd Dove-SkyScripts
```

### 2. Táº¡o mÃ´i trÆ°á»ng

```bash
# Sá»­ dá»¥ng conda (recommended)
conda create -n dove python=3.9
conda activate dove

# Hoáº·c sá»­ dá»¥ng venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate  # Windows
```

### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### 4. CÃ i Ä‘áº·t PyTorch

```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CPU only
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

---

## ğŸ“Š Dataset Setup

### Download SkyScript Dataset

```bash
# Download script
bash download_skyscript.sh

# Hoáº·c download thá»§ cÃ´ng tá»« cÃ¡c nguá»“n sau:
# - Training data (top 50%): SkyScript_train_top50pct_filtered_by_CLIP_openai.csv
# - Validation data: SkyScript_val_5K_filtered_by_CLIP_openai.csv
# - Test data: SkyScript_test_30K_filtered_by_CLIP_openai.csv
```

### Cáº¥u trÃºc thÆ° má»¥c

```
Dove-SkyScripts/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ images2/
â”‚   â”‚   â”œâ”€â”€ images3/
â”‚   â”‚   â”œâ”€â”€ ... 
â”‚   â”‚   â””â”€â”€ images7/
â”‚   â””â”€â”€ dataframe/
â”‚       â”œâ”€â”€ SkyScript_train_top50pct_filtered_by_CLIP_openai.csv
â”‚       â”œâ”€â”€ SkyScript_val_5K_filtered_by_CLIP_openai.csv
â”‚       â””â”€â”€ SkyScript_test_30K_filtered_by_CLIP_openai.csv
â”œâ”€â”€ src/
â”‚   â””â”€â”€ dove/
â”‚       â”œâ”€â”€ models/
â”‚       â”œâ”€â”€ datasets/
â”‚       â””â”€â”€ utils/
â”œâ”€â”€ runs/
â”œâ”€â”€ train_dove.py
â”œâ”€â”€ eval_retrieval.py
â””â”€â”€ demo_dove_retrieval.ipynb
```

### Unzip images

```bash
bash unzip_skyscript.sh
```

---

## ğŸ“ Training

### Basic Training

```bash
python train_dove.py \
    --data_root "data/images" \
    --train_csv "data/dataframe/SkyScript_train_top50pct_filtered_by_CLIP_openai.csv" \
    --val_csv "data/dataframe/SkyScript_val_5K_filtered_by_CLIP_openai.csv" \
    --caption_field "title_multi_objects" \
    --image_size 256 \
    --center_box_policy "none" \
    --batch_size 64 \
    --epochs 20 \
    --lr 2e-4 \
    --weight_decay 0.05 \
    --embed_dim 512 \
    --lambda_g 10.0 \
    --margin 0.2 \
    --random_rotation 1 \
    --num_workers 4 \
    --amp 1 \
    --out_dir "runs/dove_skyscript"
```

### Distributed Training (Multi-GPU)

```bash
torchrun --nproc_per_node=4 train_dove.py \
    --data_root "data/images" \
    --train_csv "data/dataframe/SkyScript_train_top50pct_filtered_by_CLIP_openai.csv" \
    --val_csv "data/dataframe/SkyScript_val_5K_filtered_by_CLIP_openai.csv" \
    --caption_field "title_multi_objects" \
    --batch_size 128 \
    --epochs 20 \
    --out_dir "runs/dove_skyscript_ddp"
```

### Training Parameters

| Parameter | Description | Default | Recommended |
|-----------|-------------|---------|-------------|
| `--data_root` | ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a images | - | `data/images` |
| `--train_csv` | CSV file cho training | - | Required |
| `--val_csv` | CSV file cho validation | - | Required |
| `--caption_field` | TrÆ°á»ng caption trong CSV | `title` | `title_multi_objects` |
| `--image_size` | KÃ­ch thÆ°á»›c áº£nh input | 256 | 256 |
| `--batch_size` | Batch size | 64 | 64-128 |
| `--epochs` | Sá»‘ epochs | 20 | 20-50 |
| `--lr` | Learning rate | 2e-4 | 2e-4 |
| `--embed_dim` | Embedding dimension | 512 | 512 |
| `--lambda_g` | Global constraint weight | 10.0 | 10.0 |
| `--margin` | Triplet loss margin | 0.2 | 0.2 |
| `--amp` | Mixed precision training | 0 | 1 |

---

## ğŸ“ˆ Evaluation

### Cross-Modal Retrieval

```bash
python eval_retrieval.py \
    --data_root "data/images" \
    --csv "data/dataframe/SkyScript_test_30K_filtered_by_CLIP_openai.csv" \
    --ckpt "runs/dove_skyscript/best.pt" \
    --caption_field "title_multi_objects" \
    --center_box_policy "none" \
    --image_size 256 \
    --batch_size 128
```

### Demo Notebook

Sá»­ dá»¥ng Jupyter notebook Ä‘á»ƒ test retrieval:

```bash
jupyter notebook demo_dove_retrieval.ipynb
```

<div align="center">
<img src="docs/images/retrieval_demo.png" width="800">
<p><i>HÃ¬nh 4: Demo cross-modal retrieval</i></p>
</div>

---

## ğŸ“Š Results

### Zero-Shot Scene Classification

| Model | AID | EuroSAT | fMoW | MillionAID | PatternNet | RESISC45 | RSI-CB | **Avg** |
|-------|-----|---------|------|------------|------------|----------|--------|---------|
| CLIP-original | 55.06 | 69.25 | 41.89 | 26.19 | 57.88 | 71.39 | 66.70 | 53.76 |
| RemoteCLIP | 34.40 | 70.85 | 27.81 | 16.77 | 47.20 | 61.91 | 74.31 | 49.95 |
| CLIP-laion-RS | 58.81 | 71.70 | 54.30 | 27.21 | 60.77 | 72.68 | 71.21 | 57.87 |
| **DOVE-50** | **70.89** | **71.70** | **51.33** | **27.12** | **67.45** | **80.88** | **70.94** | **59.93** |

### Cross-Modal Retrieval (Mean Recall %)

| Model | RSICD | RSITMD | UCM-Captions |
|-------|-------|--------|--------------|
|       | i2t / t2i | i2t / t2i | i2t / t2i |
| AMFMN* | 14.62 / 18.21 | 25.74 / 33.69 | 43.65 / 48.51 |
| GaLR* | 19.16 / 18.77 | 29.65 / 33.17 | - / - |
| CLIP-original | 19.67 / 13.84 | 27.51 / 24.10 | 68.41 / 56.76 |
| **DOVE** | **23.70 / 19.97** | **30.75 / 30.58** | **72.22 / 59.33** |

*Supervised models (seen benchmark datasets during training)

### Fine-Grained Classification (Top-1 Accuracy %)

| Model | Roof Shape | Road Smoothness | Road Surface |
|-------|------------|-----------------|--------------|
| CLIP-original | 37.50 | 25.40 | 42.73 |
| **DOVE** | **46.83** | **35.80** | **67.50** |

---

## ğŸ¯ Use Cases

### 1. Zero-Shot Classification

```python
from dove import DOVE
import torch
from PIL import Image

# Load model
model = DOVE.from_pretrained("runs/dove_skyscript/best.pt")
model.eval()

# Load image
image = Image.open("example.jpg")

# Define classes
classes = ["airport", "beach", "bridge", "farmland", "forest"]

# Predict
with torch.no_grad():
    probs = model.classify(image, classes)
    pred_class = classes[probs.argmax()]
    
print(f"Predicted: {pred_class} (confidence: {probs.max():.2%})")
```

### 2. Image-Text Retrieval

```python
# Image to Text
image = Image.open("satellite.jpg")
top_texts = model.image_to_text(image, text_database, top_k=5)

# Text to Image  
query = "airport with multiple runways"
top_images = model.text_to_image(query, image_database, top_k=5)
```

### 3. Feature Extraction

```python
# Extract visual features
image_features = model.encode_image(image)

# Extract text features
text_features = model.encode_text("residential area with roads")

# Compute similarity
similarity = torch.cosine_similarity(image_features, text_features)
```

---

## ğŸ“ Code Structure

```
src/dove/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ dove.py              # Main DOVE model
â”‚   â”œâ”€â”€ encoders.py          # MSV & RoI encoders
â”‚   â”œâ”€â”€ roam.py              # ROAM module
â”‚   â””â”€â”€ dtga.py              # DTGA module
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ skyscript.py         # SkyScript dataset loader
â”‚   â””â”€â”€ transforms.py        # Data augmentation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics.py           # Evaluation metrics
â”‚   â”œâ”€â”€ losses.py            # Loss functions
â”‚   â””â”€â”€ visualization.py     # Visualization utilities
â””â”€â”€ config.py                # Configuration
```

---

## ğŸ”¬ Ablation Studies

### áº¢nh hÆ°á»Ÿng cá»§a Î»_g (Global Constraint Weight)

<div align="center">
<img src="docs/images/lambda_g_ablation.png" width="600">
<p><i>HÃ¬nh 5: Retrieval performance vá»›i cÃ¡c giÃ¡ trá»‹ Î»_g khÃ¡c nhau</i></p>
</div>

### áº¢nh hÆ°á»Ÿng cá»§a DTGA Module

| Input Combination | Sentence Retrieval | Image Retrieval | mR |
|-------------------|-------------------|-----------------|-----|
| H^f, H^f | 15.93 / 33.19 / 46.68 | 13.50 / 44.20 / 64.51 | 36.33 |
| H^b, H^b | 15.27 / 36.06 / 50.44 | 13.89 / 44.82 / 66.11 | 37.77 |
| **H^f, H^b** | **17.04 / 39.60 / 50.88** | **13.63 / 45.27 / 66.11** | **38.75** |

---

## ğŸ¤ Contributing

ChÃºng tÃ´i welcome contributions! Vui lÃ²ng:

1. Fork repository
2. Táº¡o feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

---

## ğŸ“ Citation

Náº¿u báº¡n sá»­ dá»¥ng DOVE hoáº·c SkyScript trong nghiÃªn cá»©u, vui lÃ²ng cite:

### DOVE Model

```bibtex
@article{ma2024dove,
  title={Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval},
  author={Ma, Qing and Pan, Jiancheng and Bai, Cong},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={62},
  pages={1--14},
  year={2024},
  publisher={IEEE}
}
```

### SkyScript Dataset

```bibtex
@article{wang2023skyscript,
  title={SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing},
  author={Wang, Zhecheng and Prabha, Rajanie and Huang, Tianyuan and Wu, Jiajun and Rajagopal, Ram},
  journal={arXiv preprint arXiv:2312.12856},
  year={2023}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **DOVE Model**: Based on research by Ma et al. (IEEE TGRS 2024)
- **SkyScript Dataset**: Developed by Wang et al. (AAAI 2024)
- **Pretrained Models**: ResNet-50 pretrained on AID dataset
- **Framework**: Built on PyTorch and OpenCLIP

---

## ğŸ“® Contact

- **Issues**: [GitHub Issues](https://github.com/AlanKhan145/Dove-SkyScripts/issues)
- **Discussions**: [GitHub Discussions](https://github.com/AlanKhan145/Dove-SkyScripts/discussions)
- **Email**: [your-email@example.com](mailto:your-email@example.com)

---

## ğŸ”— Related Projects

- [SkyScript Official](https://github.com/wangzhecheng/SkyScript) - Official SkyScript dataset repository
- [RemoteCLIP](https://github.com/ChenDelong1999/RemoteCLIP) - Vision Language Foundation Model for RS
- [RSICD](https://github.com/201528014227051/RSICD_optimal) - Remote Sensing Image Captioning Dataset
- [RSITMD](https://github.com/xiaoyuan1996/AMFMN) - Fine-grained Remote Sensing Dataset

---

<div align="center">

**â­ Náº¿u project nÃ y há»¯u Ã­ch, hÃ£y cho chÃºng tÃ´i má»™t star! â­**

Made with â¤ï¸ by Remote Sensing Community

</div>
